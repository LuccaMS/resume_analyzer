{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c04feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, uuid\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3967de",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"backend/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d167b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = os.listdir(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b928e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab25927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in lista:\n",
    "    file_to_open = f\"{var}/{path}\"\n",
    "    with open(file_to_open) as file:\n",
    "        data = json.load(file)\n",
    "        rec_texts.append(data.get(\"rec_texts\"))\n",
    "    \n",
    "    os.remove(file_to_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c88db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class ResumeData(BaseModel):\n",
    "    # Personal Information\n",
    "    full_name: Optional[str] = None\n",
    "    current_position: Optional[str] = None\n",
    "    \n",
    "    # Contact Information\n",
    "    email: Optional[str] = None\n",
    "    phone: Optional[str] = None\n",
    "    linkedin: Optional[str] = None\n",
    "    github: Optional[str] = None\n",
    "    address: Optional[str] = None\n",
    "    \n",
    "    # Professional Summary\n",
    "    professional_summary: Optional[str] = None\n",
    "    \n",
    "    # Work Experience (list of dictionaries)\n",
    "    work_experience: List[str]  = []\n",
    "    \n",
    "    # Education (list of dictionaries)\n",
    "    education: List[str] = []\n",
    "    \n",
    "    # Skills\n",
    "    technical_skills: List[str] = []\n",
    "    soft_skills: List[str] = []\n",
    "    \n",
    "    # Additional Sections\n",
    "    certifications: List[str] = []\n",
    "    projects: List[str] = []\n",
    "    languages: List[str] = []\n",
    "    achievements: List[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2061f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_EXTRACTION_PROMPT = \"\"\"\n",
    "You are a resume information extraction specialist. You will receive a list of strings from OCR (Optical Character Recognition) processing of a resume document.\n",
    "\n",
    "IMPORTANT: OCR data often contains errors such as:\n",
    "- Words concatenated without spaces (e.g., \"scalableAWSinfrastructuresupportingLLMoperationsthrough\")\n",
    "- Missing spaces between words, sentences, or sections\n",
    "- Words cut off or split incorrectly\n",
    "- Inconsistent formatting and spacing\n",
    "- Some text may be garbled or incomplete\n",
    "\n",
    "Your task is to extract structured information from this imperfect OCR data and return a JSON object that matches the provided schema.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Carefully read through all OCR text lines to understand the resume structure\n",
    "2. Use context clues to separate concatenated words and fix spacing issues\n",
    "3. Extract information even if it's imperfect - do your best to interpret the meaning\n",
    "4. For work experience, try to identify job titles, company names, dates, and responsibilities\n",
    "5. Look for education information including degrees and institutions\n",
    "6. Extract technical skills, programming languages, tools, and technologies mentioned\n",
    "7. If information is unclear or missing, set the field to null or empty list as appropriate\n",
    "8. Return only valid JSON - no additional text or explanations\n",
    "\n",
    "OCR Data to process:\n",
    "{ocr_data}\n",
    "\n",
    "Return the extracted information as a JSON object:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85353099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "#final_prompt = RESUME_EXTRACTION_PROMPT.format(ocr_data=ocr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf09169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a resume information extraction specialist. You will receive a list of strings from OCR (Optical Character Recognition) processing of a resume document.\n",
      "\n",
      "IMPORTANT: OCR data often contains errors such as:\n",
      "- Words concatenated without spaces (e.g., \"scalableAWSinfrastructuresupportingLLMoperationsthrough\")\n",
      "- Missing spaces between words, sentences, or sections\n",
      "- Words cut off or split incorrectly\n",
      "- Inconsistent formatting and spacing\n",
      "- Some text may be garbled or incomplete\n",
      "\n",
      "Your task is to extract structured information from this imperfect OCR data and return a JSON object that matches the provided schema.\n",
      "\n",
      "INSTRUCTIONS:\n",
      "1. Carefully read through all OCR text lines to understand the resume structure\n",
      "2. Use context clues to separate concatenated words and fix spacing issues\n",
      "3. Extract information even if it's imperfect - do your best to interpret the meaning\n",
      "4. For work experience, try to identify job titles, company names, dates, and responsibilities\n",
      "5. Look for education information including degrees and institutions\n",
      "6. Extract technical skills, programming languages, tools, and technologies mentioned\n",
      "7. If information is unclear or missing, set the field to null or empty list as appropriate\n",
      "8. Return only valid JSON - no additional text or explanations\n",
      "\n",
      "OCR Data to process:\n",
      "['Lucca Machado', '+55(51）98033-3041', 'github.com/LuccaMS', 'luccamachado16@gmail.com', 'Data Scientist', 'linkedin.com/in/luccams', 'Data Scientist and Computer Engineer from UFSC,with experience in developing conversational agents and Al-based solutions', 'have solidexpertiei RESTul APIsdataprocssing,ETL,andNLPuingPthonSQLandtoolslikeFastAP,treamliLaC', 'LangGraph,aswell as various AWS services.', 'SKILLS AND CERTIFICATIONS', 'Tools', 'Python,GitLangCainLngGrah,AWS,NLP,FastApireami', 'Certifications', 'AWS Certified Al Practitioner', 'TECHNICALEXPERIENCE', 'Data Scientist', '07/2024—Now', 'A3Data', 'Remote', 'Led theimprovement ofa Text-to-SQL agent-based LLM system,successfully achieving an accuracyof95%,resulting in a30%', 'increase.', 'Designed and maintained scalableAWSinfrastructuresupportingLLMoperationsthrough Terraform-basedinfrastructure as', 'Participated in the developmentofaconversational agentcapableofinterpreting user inputs to generate DSL queries in external', 'code practices', 'APls, building dynamic and personalized responses for users.', 'Developed agraph-based conversational agent (LangGraph and LangChain) integrated with aproductdatabase using RAG', 'guiding users through the purchasing process and efficiently optimizing the sales funnel.', 'DeployedmodelsonAWS EC2 and ECSusing Dockerensuring scalability and reliability;developed guardrails using', 'LLM-as-a-Judge for secure Al interactions.', 'Applied advanced prompt engineering techniques,such asfew-shot learning,negative prompting,and zero-shot learning,to', 'optimize model performance.Conducted ETL processes to structure data for building an RAG system.', 'Internship', '11/2023—06/2024', 'BTG Pactual Bank', 'Remote', 'Successfully manipulated and processed data in spreadsheets,enabling automated distribution to thousands ofclients while', 'ensuring precision and scalability.', 'Developed RESTful APls using FastAPl and MySoL,achieving efficient integration between systems and databases.', 'Created Aws Lambda functions for large-scale email stream processing,optimizing cloud-based infrastructure.', 'Integrated APlswithfront-endinterfacesusing ReactandPython,enhancing userexperience and applicationfunctionality', 'Modeledandimplemented new relationalSQLdatabase tables,ensuringefficientdatastructuringandconsistency.', 'Machine Learning Intern', '01/2022-10/2023', 'Universidade Federal de Santg Cataring', 'Successfully analyzed andcleaneddata usinglibrariessuchas PandasSeaborn,and Matplotlib,ensuringhigh-quality dataand', 'effective visualization.', 'Implemented machine learningmodels usingframeworks like PyTorch,TensorFlow,scikit-learnandNumPy,optimizingthe', 'accuracy and performance of solutions.', 'Developedintelligent chatbotsusing Pythonand advanced NaturalLanguageProcessing(NLP)techniques,enabling', 'personalized and interactive user experiences.', 'Created RESTful APlsforchatbotintegrationusingFastAPl,achievingefficientcommunicationbetween systems andinterfaces.', 'DesignedinteractivedashboardsusingtoolslikePowerBTableauandPython,empoweringdata-drivendecision-making', 'processes.', 'EDUCATION', 'Computer Engineering, Universidade Federal de Santa Catarina', '2019-2024']\n",
      "\n",
      "Return the extracted information as a JSON object:\n",
      "\n",
      "\n",
      "You are a resume information extraction specialist. You will receive a list of strings from OCR (Optical Character Recognition) processing of a resume document.\n",
      "\n",
      "IMPORTANT: OCR data often contains errors such as:\n",
      "- Words concatenated without spaces (e.g., \"scalableAWSinfrastructuresupportingLLMoperationsthrough\")\n",
      "- Missing spaces between words, sentences, or sections\n",
      "- Words cut off or split incorrectly\n",
      "- Inconsistent formatting and spacing\n",
      "- Some text may be garbled or incomplete\n",
      "\n",
      "Your task is to extract structured information from this imperfect OCR data and return a JSON object that matches the provided schema.\n",
      "\n",
      "INSTRUCTIONS:\n",
      "1. Carefully read through all OCR text lines to understand the resume structure\n",
      "2. Use context clues to separate concatenated words and fix spacing issues\n",
      "3. Extract information even if it's imperfect - do your best to interpret the meaning\n",
      "4. For work experience, try to identify job titles, company names, dates, and responsibilities\n",
      "5. Look for education information including degrees and institutions\n",
      "6. Extract technical skills, programming languages, tools, and technologies mentioned\n",
      "7. If information is unclear or missing, set the field to null or empty list as appropriate\n",
      "8. Return only valid JSON - no additional text or explanations\n",
      "\n",
      "OCR Data to process:\n",
      "['Larissa Pereira', 'Frontend Developer at Codewave', 'Email: larissa.pereira@email.com |Phone: (11)98748-1868', 'Professional Summary', 'Creative Frontend Developer specializing in React and responsive design with a passion for clean', 'UI.', 'Professional Experience', 'Frontend Developer - InovaData (2020-2021)', '- Developed and maintained systems.', '- Collaborated with teams.', '- Delivered projects on time.', 'Frontend Developer - SecureX (2021-2022)', '- Developed and maintained systems.', '- Collaborated with teams.', '- Delivered projects on time.', 'Education', 'Bachelor of Computer Science - Federal Institute of Technology (2016 - 2020)', 'Technical Skills', 'HTML, CSS,JavaScript, React, Tailwind CSS, Figma']\n",
      "\n",
      "Return the extracted information as a JSON object:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in rec_texts:\n",
    "    prompt = RESUME_EXTRACTION_PROMPT.format(ocr_data=item)\n",
    "\n",
    "    client = genai.Client(api_key=\"AIzaSyBNY0Ys8BqiMnPQ6ajbmtfgqqjcjMf0VLw\")\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_schema\": ResumeData,\n",
    "    })\n",
    "\n",
    "    res = json.loads(response.text)\n",
    "\n",
    "    if res[\"full_name\"] is not None:\n",
    "        # Convert to lowercase\n",
    "        file_name = res[\"full_name\"].lower()\n",
    "        # Remove spaces\n",
    "        file_name = file_name.replace(\" \", \"\")\n",
    "        # Remove special characters using regex (keeps only alphanumeric characters)\n",
    "        file_name = re.sub(r'[^a-z0-9]', '', file_name)\n",
    "    else:\n",
    "        file_name = str(uuid.uuid4())\n",
    "\n",
    "    with open(f\"{file_name}.json\", \"w\") as json_file:\n",
    "        json.dump(res,json_file,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff164821",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = RESUME_EXTRACTION_PROMPT.format(ocr_data=rec_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d077d283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"full_name\": \"Lucca Machado\",\n",
      "  \"current_position\": \"Data Scientist\",\n",
      "  \"email\": \"luccamachado16@gmail.com\",\n",
      "  \"phone\": \"+55(51)98033-3041\",\n",
      "  \"linkedin\": \"linkedin.com/in/luccams\",\n",
      "  \"github\": \"github.com/LuccaMS\",\n",
      "  \"address\": null,\n",
      "  \"professional_summary\": \"Data Scientist and Computer Engineer from UFSC, with experience in developing conversational agents and AI-based solutions. Have solid expertise in RESTful APIs, data processing, ETL, and NLP using Python, SQL and tools like FastAPI, Streamlit, LangChain, LangGraph, as well as various AWS services.\",\n",
      "  \"work_experience\": [\n",
      "    \"Data Scientist, A3Data, Remote, 07/2024—Now. Led the improvement of a Text-to-SQL agent-based LLM system, successfully achieving an accuracy of 95%, resulting in a 30% increase. Designed and maintained scalable AWS infrastructure supporting LLM operations through Terraform-based infrastructure as code practices. Participated in the development of a conversational agent capable of interpreting user inputs to generate DSL queries in external APIs, building dynamic and personalized responses for users. Developed a graph-based conversational agent (LangGraph and LangChain) integrated with a product database using RAG, guiding users through the purchasing process and efficiently optimizing the sales funnel. Deployed models on AWS EC2 and ECS using Docker, ensuring scalability and reliability; developed guardrails using LLM-as-a-Judge for secure AI interactions. Applied advanced prompt engineering techniques, such as few-shot learning, negative prompting, and zero-shot learning, to optimize model performance. Conducted ETL processes to structure data for building an RAG system.\",\n",
      "    \"Internship, BTG Pactual Bank, Remote, 11/2023—06/2024. Successfully manipulated and processed data in spreadsheets, enabling automated distribution to thousands of clients while ensuring precision and scalability. Developed RESTful APIs using FastAPI and MySQL, achieving efficient integration between systems and databases. Created AWS Lambda functions for large-scale email stream processing, optimizing cloud-based infrastructure. Integrated APIs with front-end interfaces using React and Python, enhancing user experience and application functionality. Modeled and implemented new relational SQL database tables, ensuring efficient data structuring and consistency.\",\n",
      "    \"Machine Learning Intern, Universidade Federal de Santa Catarina, 01/2022-10/2023. Successfully analyzed and cleaned data using libraries such as Pandas, Seaborn, and Matplotlib, ensuring high-quality data and effective visualization. Implemented machine learning models using frameworks like PyTorch, TensorFlow, scikit-learn, and NumPy, optimizing the accuracy and performance of solutions. Developed intelligent chatbots using Python and advanced Natural Language Processing (NLP) techniques, enabling personalized and interactive user experiences. Created RESTful APIs for chatbot integration using FastAPI, achieving efficient communication between systems and interfaces. Designed interactive dashboards using tools like Power BI, Tableau, and Python, empowering data-driven decision-making processes.\"\n",
      "  ],\n",
      "  \"education\": [\n",
      "    \"Computer Engineering, Universidade Federal de Santa Catarina, 2019-2024\"\n",
      "  ],\n",
      "  \"technical_skills\": [\n",
      "    \"Python\",\n",
      "    \"Git\",\n",
      "    \"LangChain\",\n",
      "    \"LangGraph\",\n",
      "    \"AWS\",\n",
      "    \"NLP\",\n",
      "    \"FastAPI\",\n",
      "    \"Streamlit\",\n",
      "    \"SQL\",\n",
      "    \"Terraform\",\n",
      "    \"Docker\",\n",
      "    \"Prompt Engineering\",\n",
      "    \"Pandas\",\n",
      "    \"Seaborn\",\n",
      "    \"Matplotlib\",\n",
      "    \"PyTorch\",\n",
      "    \"TensorFlow\",\n",
      "    \"scikit-learn\",\n",
      "    \"NumPy\",\n",
      "    \"React\",\n",
      "    \"Power BI\",\n",
      "    \"Tableau\",\n",
      "    \"MySQL\"\n",
      "  ],\n",
      "  \"soft_skills\": [],\n",
      "  \"certifications\": [\n",
      "    \"AWS Certified AI Practitioner\"\n",
      "  ],\n",
      "  \"projects\": [],\n",
      "  \"languages\": [],\n",
      "  \"achievements\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=\"AIzaSyBNY0Ys8BqiMnPQ6ajbmtfgqqjcjMf0VLw\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=final_prompt,\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_schema\": ResumeData,\n",
    "    }\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aacf9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa59cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if res[\"full_name\"] is not None:\n",
    "    # Convert to lowercase\n",
    "    file_name = res[\"full_name\"].lower()\n",
    "    # Remove spaces\n",
    "    file_name = file_name.replace(\" \", \"\")\n",
    "    # Remove special characters using regex (keeps only alphanumeric characters)\n",
    "    file_name = re.sub(r'[^a-z0-9]', '', file_name)\n",
    "else:\n",
    "    file_name = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7af75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{file_name}.json\", \"w\") as json_file:\n",
    "    json.dump(res,json_file,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e4bae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'luccamachado'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
